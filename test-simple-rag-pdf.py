import argparse
import os
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_experimental.text_splitter import SemanticChunker
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaLLM
from langchain.prompts import PromptTemplate
# from langchain.schema.runnable import RunnableLambda
# from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableMap
from langchain.chains import RetrievalQA

def process_pdf(pdf_path):
    # PDF í…ìŠ¤íŠ¸ ë¡œë“œ
    loader = PDFPlumberLoader(pdf_path)
    docs = loader.load()

    # í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ì  ì²­í¬ë¡œ ë¶„í• 
    text_splitter = SemanticChunker(HuggingFaceEmbeddings())
    documents = text_splitter.split_documents(docs)

    # ì„ë² ë”© ìƒì„±
    embeddings = HuggingFaceEmbeddings()
    vector = FAISS.from_documents(documents, embeddings)
    retriever = vector.as_retriever(search_type="similarity", search_kwargs={"k": 3})

    return retriever

def setup_qa_chain(retriever):
    # ëª¨ë¸ ì„ ì •
    llm = OllamaLLM(model="deepseek-r1:1.5b")

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‘ì„±
    # í”„ë¡¬í”„íŠ¸ë“¤ì€ json ìœ¼ë¡œ ì €ì¥ í›„ ë¡œë“œ ê°€ëŠ¥ ( ì±… p.61 ì°¸ê³  )
    prompt = """
    1. ì•„ë˜ ë§¥ë½ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
    2. í™•ì‹¤í•˜ì§€ ì•Šì€ ê²½ìš° "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•˜ì„¸ìš”.
    3. ë‹µë³€ì€ 4ë¬¸ì¥ì„ ë„˜ì§€ ì•Šë„ë¡ í•˜ì„¸ìš”.

    ë§¥ë½: {context}

    ì§ˆë¬¸: {question}

    ë‹µë³€:
    """
    QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt) # PromptTemplate ì´ˆê¸°í™” ê³¼ì • ( input_variables ì§€ì • ì•ˆí•´ë„ ë¨ )

    # ì²´ì¸ 1: ë‹µë³€ ìƒì„±
    # llm_chain = QA_CHAIN_PROMPT | llm

    # ì²´ì¸ 2: ë¬¸ì„œ ì²­í¬ ê²°í•©
    # document_prompt = PromptTemplate(
    #     input_variables=["page_content", "source"],
    #     template="ë§¥ë½:\ncontent:{page_content}\nsource:{source}"
    # )

    # combine_documents_chain = StuffDocumentsChain(
    #     llm_chain=llm_chain,
    #     document_variable_name="context",
    #     document_prompt=document_prompt
    # )
    
    # return RetrievalQA(
    #     combine_documents_chain=combine_documents_chain,
    #     retriever=retriever,
    #     return_source_documents=True
    # )

    qa_chain = (
        RunnableMap({"context": retriever})  # ë¬¸ì„œ ê²€ìƒ‰
        | QA_CHAIN_PROMPT  # í”„ë¡¬í”„íŠ¸ ì ìš©
        | llm  # LLM ì‹¤í–‰
        | StrOutputParser()  # ê²°ê³¼ íŒŒì‹±
    )

    return qa_chain

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("pdf_path", type=str, help="Path to the PDF file")
    args = parser.parse_args()
    
    if not os.path.exists(args.pdf_path):
        print("Error: PDF file not found!")
        return
    
    print("Processing PDF...")
    retriever = process_pdf(args.pdf_path)
    qa_chain = setup_qa_chain(retriever)
    
    while True:
        user_input = input("PDFì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥): ")
        if user_input.lower() == "exit":
            break
        
        # response = qa_chain(user_input)["result"]
        # ğŸ”¹ ê²€ìƒ‰ì„ ìœ„í•´ retrieverì— `user_input`ë§Œ ì „ë‹¬í•´ì•¼ í•¨
        relevant_docs = retriever.invoke(user_input)

        # ğŸ”¹ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ contextë¡œ í•©ì¹¨
        context = "\n".join([doc.page_content for doc in relevant_docs])

        # ğŸ”¹ ì´ì œ LLMì— `context`ì™€ `question`ì„ í•¨ê»˜ ì „ë‹¬
        response = qa_chain.invoke({"context": context, "question": user_input})
        print("Response:", response)

if __name__ == "__main__":
    main()
